1. Description of any further steps you had to take to get from the automatically constructed choice file to one that would both (a) customize and (b) compile.

2. Your answers to the questions about the initial and final [incr tsdb()] runs, for both test corpus and test suite, repeated here:
   1. How many items parsed?
   2. What is the average number of parses per parsed item?
   3. How many parses did the most ambiguous item receive?
   4. What sources of ambiguity can you identify?
   5. For 10 items (if you have at least that many parsing), do any of the parses look reasonable in the semantics? (Emily will demo in class on Tuesday.) 

3. Documentation of the phenomena you have added to your testsuite, illustrated with examples from the testsuite.

4. Documentation of the choices you made in the customization system, illustrated with examples from your test suite.
        This can be interleaved with the documentation of the phenomena (so you describe each phenomenon and then the choices you used to add an analysis of it to the grammar), but the documentation of the phenomenon and choices should be logically separate. Here's an example of what this should look like.

5. Descriptions of any properties of your language illustrated in your test suite but not covered by your starter grammar and/or the customization system.

6. If you have identified ways (other than those you reported last week) in which the automatically created choices file is particularly off-base, please report them here. If you can include IGT from the testsuite or your descriptive materials illustrating the problem, that is even better. 
